{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from PIL import Image\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_generator import DataGenerator\n",
    "\n",
    "# Create train, validation, and test data generators\n",
    "train_data_gen = DataGenerator(os.getenv('TRAIN_DATA_DIR'))\n",
    "train_data_gen.summary()\n",
    "\n",
    "val_data_gen = DataGenerator(os.getenv('VAL_DATA_DIR'))\n",
    "val_data_gen.summary()\n",
    "\n",
    "test_data_gen = DataGenerator(os.getenv('TEST_DATA_DIR'))\n",
    "test_data_gen.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from keras import Sequential\n",
    "from keras.layers import RandomFlip, RandomRotation, RandomZoom\n",
    "\n",
    "# Create data augmentator\n",
    "data_aug = Sequential(name='data_augmentation')\n",
    "data_aug.add(RandomFlip(name='random_flip'))\n",
    "data_aug.add(RandomRotation(0.05, fill_mode='nearest', name='random_rotation'))\n",
    "data_aug.add(RandomZoom(0.1, fill_mode='nearest', name='random_zoom'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation examples\n",
    "fig, axes = plt.subplots(5, 2)\n",
    "\n",
    "# Get all the paths to the our images from the data generator\n",
    "paths = train_data_gen.get_paths()\n",
    "\n",
    "for i in range(5):\n",
    "    with Image.open(paths[i]) as img:\n",
    "        axes[i,0].imshow(img)\n",
    "        axes[i,0].set_axis_off()\n",
    "        # Augment image with data_aug\n",
    "        img = np.array(img)\n",
    "        img = np.expand_dims(img, axis=-1)\n",
    "        img_aug = data_aug(img).numpy()\n",
    "        axes[i,1].imshow(img_aug.astype('int32'))\n",
    "        axes[i,1].set_axis_off()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Input\n",
    "from keras.applications import Xception\n",
    "from keras.layers import GlobalMaxPooling2D, Dense\n",
    "\n",
    "# Load Xception model and set weights to not trainable\n",
    "xception = Xception(include_top=False, weights='imagenet')\n",
    "xception.trainable = False\n",
    "\n",
    "# Store the number of classes and input shape\n",
    "num_classes = len(train_data_gen.labels)\n",
    "input_shape = train_data_gen.input_shape + (3,) \n",
    "\n",
    "# Create model\n",
    "def create_model(name):\n",
    "\tmodel = Sequential(name=name)\n",
    "\tmodel.add(Input(input_shape))\n",
    "\tmodel.add(xception)\n",
    "\tmodel.add(GlobalMaxPooling2D(name='global_pooling'))\n",
    "\tmodel.add(Dense(num_classes, activation='softmax', name='output_layer'))\n",
    "\treturn model\n",
    "\n",
    "model = create_model('phneumonia_classifier')\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "from keras.losses import CategoricalCrossentropy\n",
    "from keras.metrics import CategoricalAccuracy\n",
    "\n",
    "# Compile model\n",
    "model.compile(\n",
    "    loss=CategoricalCrossentropy(),\n",
    "    optimizer=Adam(),\n",
    "    metrics=[CategoricalAccuracy()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.xception import preprocess_input\n",
    "\n",
    "# Add the preprocess_input func to our data generators\n",
    "train_data_gen.preproc_func = preprocess_input\n",
    "val_data_gen.preproc_func = preprocess_input\n",
    "test_data_gen.preproc_func = preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "history = model.fit(\n",
    "    x=train_data_gen,\n",
    "    epochs=100,\n",
    "    batch_size=train_data_gen.batch_size,\n",
    "    validation_data=val_data_gen,\n",
    "    validation_batch_size=val_data_gen.batch_size,\n",
    "    # Earlystopping stops training when val_loss stops improving\n",
    "    callbacks=[EarlyStopping(monitor='val_loss', patience=3)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history, *args):\n",
    "    for arg in args:\n",
    "        plt.plot(history.history[arg], label=f\"{arg}\")\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history, 'loss', 'val_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history, 'categorical_accuracy', 'val_categorical_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data_gen.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remark:\n",
    "The validation data generator only contains 16 data points. Lets create a more representive validation data generator by partitioning out training data generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new validation data generator with 20 percent of the training data\n",
    "val_data_gen = train_data_gen.partition_data_generator(0.2)\n",
    "\n",
    "train_data_gen.summary()\n",
    "val_data_gen.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DON'T LEAK DATA!\n",
    "Since our model was fitted with some of validation data included in the new validation data generator, we must recreate our model so that the weights have not been fitted with any of the validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recreate model\n",
    "model = create_model('phneumonia_classifier')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "model.compile(\n",
    "    loss=CategoricalCrossentropy(),\n",
    "    optimizer=Adam(),\n",
    "    metrics=[CategoricalAccuracy()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    x=train_data_gen,\n",
    "    epochs=100,\n",
    "    batch_size=train_data_gen.batch_size,\n",
    "    validation_data=val_data_gen,\n",
    "    validation_batch_size=val_data_gen.batch_size,\n",
    "    # Earlystopping stops training when val_loss stops improving\n",
    "    callbacks=[EarlyStopping(monitor='val_loss', patience=3)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history, 'loss', 'val_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history, 'categorical_accuracy', 'val_categorical_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(test_data_gen);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data augmentation layers and preprocessing function\n",
    "def data_aug_and_preproc(inputs):\n",
    "\tz = RandomFlip(name='random_flip')(inputs)\n",
    "\tz = RandomRotation(0.05, fill_mode='nearest', name='random_rotation')(z)\n",
    "\tz = RandomZoom(0.1, fill_mode='nearest', name='random_zoom')(z)\n",
    "\treturn preprocess_input(z)\n",
    "\n",
    "# Add data augmentation and preprocessing function to the training data generator\n",
    "train_data_gen.preproc_func = data_aug_and_preproc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    x=train_data_gen,\n",
    "    epochs=100,\n",
    "    batch_size=train_data_gen.batch_size,\n",
    "    validation_data=val_data_gen,\n",
    "    validation_batch_size=val_data_gen.batch_size,\n",
    "    # Changed patience to 10 so that our model is trained on more\n",
    "    # augmented data\n",
    "    callbacks=[EarlyStopping(monitor='val_loss', patience=10)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history, 'loss', 'val_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(test_data_gen);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DSI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
